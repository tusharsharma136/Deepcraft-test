# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KSCXe9gEmO-MqYt0_hrOA6N8ufCJy1Cp

#1. Data Understanding and EDA

Loading and Inspecting Data
"""

import pandas as pd

data=pd.read_csv("stock_price.csv")
data.head()

# Check for missing values
print(data.isnull().sum())

# Summary statistics
print(data.describe())

import pandas as pd

# Renaming the columns to English
data.columns = ['Date', 'Closing', 'Opening', 'High', 'Low', 'Volume', 'Change%']

data.head()

"""#2. Data Preprocessing and Feature Engineering

Handling Missing Values
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(data[['Closing', 'Opening', 'High', 'Low', 'Volume']].corr(), annot=True, cmap='coolwarm')
plt.show()

data['Change%'] = data['Change%'].str.replace('%','').astype(float)
plt.figure(figsize=(8, 5))
plt.hist(data['Change%'], bins=50, color='blue')
plt.title('Distribution of Daily Percentage Price Change')
plt.xlabel('Daily % Change')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(data['Date'], data['Volume'], color='orange', label='Volume')
plt.xticks(rotation=45)
plt.title('Trading Volume Over Time')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.legend()
plt.show()

import plotly.graph_objects as go

fig = go.Figure(data=[go.Candlestick(x=data['Date'],
                    open=data['Opening'],
                    high=data['High'],
                    low=data['Low'],
                    close=data['Closing'])])

fig.update_layout(title='Candlestick Chart for Stock Prices', xaxis_title='Date', yaxis_title='Price')
fig.show()

data['MA_20'] = data['Closing'].rolling(window=20).mean()
data['MA_50'] = data['Closing'].rolling(window=50).mean()

plt.figure(figsize=(12, 6))
plt.plot(data['Date'], data['Closing'], label='Closing Price', color='blue')
plt.plot(data['Date'], data['MA_20'], label='20-Day MA', color='red')
plt.plot(data['Date'], data['MA_50'], label='50-Day MA', color='green')
plt.xticks(rotation=45)
plt.title('Stock Prices with Moving Averages')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

data['Volatility'] = data['Change%'].rolling(window=20).std()

plt.figure(figsize=(12, 6))
plt.plot(data['Date'], data['Volatility'], label='20-Day Rolling Volatility', color='purple')
plt.xticks(rotation=45)
plt.title('Rolling Volatility Over Time')
plt.xlabel('Date')
plt.ylabel('Volatility')
plt.legend()
plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(data['Closing'], period=30)  # assuming daily data with a 30-day season
decomposition.plot()
plt.show()

def RSI(series, period=14):
    delta = series.diff(1)
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=period).mean()
    avg_loss = loss.rolling(window=period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

data['RSI'] = RSI(data['Closing'], period=14)

plt.figure(figsize=(10, 5))
plt.plot(data['Date'], data['RSI'], label='RSI', color='magenta')
plt.axhline(70, linestyle='--', color='red')  # Overbought level
plt.axhline(30, linestyle='--', color='green')  # Oversold level
plt.xticks(rotation=45)
plt.title('Relative Strength Index (RSI)')
plt.xlabel('Date')
plt.ylabel('RSI')
plt.legend()
plt.show()

from pandas.plotting import lag_plot

plt.figure(figsize=(6, 6))
lag_plot(data['Closing'], lag=1)
plt.title('Lag Plot of Closing Prices')
plt.show()

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(data['Closing'], lags=30)
plot_pacf(data['Closing'], lags=30)
plt.show()

"""Visualizing the Data"""

import matplotlib.pyplot as plt

# Plotting Closing price over time
plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Closing'], label='Closing Price')
plt.title('Closing Price over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.legend()
plt.show()

"""Checking Trends, Seasonality, and Outliers"""

# Box plot for detecting outliers in the closing prices
plt.figure(figsize=(10, 6))
plt.boxplot(data['Closing'])
plt.title('Box plot of Closing Prices')
plt.show()

# Rolling Mean for Trend and Seasonality
data['Rolling_Mean_30'] = data['Closing'].rolling(window=30).mean()

plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Closing'], label='Closing Price')
plt.plot(data['Date'], data['Rolling_Mean_30'], color='red', label='30-day Rolling Mean')
plt.title('Closing Price with 30-day Rolling Mean')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# Filling missing values using forward fill method
data.fillna(method='ffill', inplace=True)

"""Feature Engineering (Lag Features and Moving Averages)"""

# Lag features: Closing price from the previous 7 days
for i in range(1, 8):
    data[f'Lag_{i}'] = data['Closing'].shift(i)

# Adding moving average features
data['MA_7'] = data['Closing'].rolling(window=7).mean()
data['MA_30'] = data['Closing'].rolling(window=30).mean()

# Dropping rows with NaN values created by lag features
data.dropna(inplace=True)

data.head()

"""Scaling Data (Normalization)"""

# Define a function to convert 'B', 'M', and 'K' in the Volume column to actual numbers
def convert_volume(value):
    value = value.strip()  # Remove any surrounding spaces
    if 'B' in value:
        return float(value.replace('B', '')) * 1e9  # Convert billions
    elif 'M' in value:
        return float(value.replace('M', '')) * 1e6  # Convert millions
    elif 'K' in value:
        return float(value.replace('K', '')) * 1e3  # Convert thousands
    else:
        return float(value)  # No suffix, just convert directly

# Apply the conversion function to the Volume column
data['Volume'] = data['Volume'].apply(convert_volume)

# Now apply the MinMaxScaler after converting the Volume column to numeric
from sklearn.preprocessing import MinMaxScaler

# Scaling closing prices between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
data[['Closing', 'Opening', 'High', 'Low', 'Volume']] = scaler.fit_transform(
    data[['Closing', 'Opening', 'High', 'Low', 'Volume']]
)

print(data.head())  # Check the scaled data

"""#3. Model Selection and Training

Model 1: ARIMA
"""

from statsmodels.tsa.arima.model import ARIMA

# Preparing data for ARIMA
arima_data = data['Closing']

# Building ARIMA model (p=5, d=1, q=0) - these are placeholder parameters
model = ARIMA(arima_data, order=(5, 1, 0))
arima_model = model.fit()

# Forecast the next 30 days
forecast = arima_model.forecast(steps=30)
print(forecast)

"""Model 2: LSTM"""

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Reshaping the data to be 3D for LSTM input
X = data[['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5', 'Lag_6', 'Lag_7']].values
y = data['Closing'].values

X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # (samples, time_steps, features)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Build LSTM model
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Predictions
predicted_stock_price = model.predict(X_test)

"""# 4. Model Evaluation and Result Analysis

ARIMA Evaluation
"""

from sklearn.metrics import mean_squared_error

# Calculate RMSE for ARIMA
test_data = arima_data[-30:]  # last 30 days
rmse = np.sqrt(mean_squared_error(test_data, forecast))
print(f'ARIMA RMSE: {rmse}')

"""LSTM Evaluation"""

# Create a separate scaler for the Closing price
closing_scaler = MinMaxScaler(feature_range=(0, 1))

# Fit this scaler only on the Closing price
data['Closing_scaled'] = closing_scaler.fit_transform(data[['Closing']])

# Now scale the predicted stock prices and test data
predicted_stock_price_rescaled = closing_scaler.inverse_transform(predicted_stock_price)
y_test_rescaled = closing_scaler.inverse_transform([y_test])

# Calculate RMSE for LSTM
rmse_lstm = np.sqrt(mean_squared_error(y_test_rescaled[0], predicted_stock_price_rescaled))
print(f'LSTM RMSE: {rmse_lstm}')

"""#5. Refinement and Model Retraining

ARIMA: Tweak p, d, and q parameters and retrain.
"""

import itertools
import warnings
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

warnings.filterwarnings("ignore")  # Ignore convergence warnings

# Define p, d, q values to test
p = range(0, 5)
d = range(0, 5)
q = range(0, 5)

# Generate all possible combinations of p, d, and q
pdq = list(itertools.product(p, d, q))

best_score, best_cfg = float("inf"), None

for param in pdq:
    try:
        model = ARIMA(data['Closing'], order=param)
        model_fit = model.fit()
        y_pred = model_fit.forecast(steps=len(test_data))
        rmse = np.sqrt(mean_squared_error(test_data, y_pred))
        if rmse < best_score:
            best_score, best_cfg = rmse, param
        print(f'ARIMA{param} RMSE={rmse:.4f}')
    except:
        continue

print(f'Best ARIMA parameters: {best_cfg} with RMSE={best_score:.4f}')

from statsmodels.tsa.arima.model import ARIMA

# Preparing data for ARIMA
arima_data = data['Closing']

# Building ARIMA model (p=2, d=2, q=0) - these are best placeholder parameters
model = ARIMA(arima_data, order=(2, 2, 0))
arima_model = model.fit()

# Forecast the next 30 days
forecast = arima_model.forecast(steps=30)
print(forecast)

"""LSTM: Add dropout layers, change the number of LSTM units or layers, and retrain."""

# Adding dropout to LSTM to improve performance
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=32)

"""#Other model testing"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

data.head()

# Convert Change% to numeric (without the percentage sign)
data['Change%'] = data['Change%'].str.replace('%', '').astype(float)

# Set 'Date' as index if not needed as a feature
data.set_index('Date', inplace=True)

# Splitting the dataset into features and target
X = data.drop(columns=['Closing'])  # Drop target column
y = data['Closing']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVR Model
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)
y_pred_svr = svr_model.predict(X_test)

# Calculate RMSE for SVR
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
print(f'SVR RMSE: {rmse_svr:.4f}')

# Random Forest Regressor Model
rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)
rfr_model.fit(X_train, y_train)
y_pred_rfr = rfr_model.predict(X_test)

# Calculate RMSE for Random Forest
rmse_rfr = np.sqrt(mean_squared_error(y_test, y_pred_rfr))
print(f'Random Forest RMSE: {rmse_rfr:.4f}')

# XGBoost Model
xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Calculate RMSE for XGBoost
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
print(f'XGBoost RMSE: {rmse_xgb:.4f}')

# Now apply the MinMaxScaler after converting the Volume column to numeric
from sklearn.preprocessing import MinMaxScaler

# Scaling closing prices between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
data[['Closing', 'Opening', 'High', 'Low', 'Volume']] = scaler.fit_transform(
    data[['Closing', 'Opening', 'High', 'Low', 'Volume']]
)

print(data.head())  # Check the scaled data

# SVR Model
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)
y_pred_svr = svr_model.predict(X_test)

# Calculate RMSE for SVR
rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))
print(f'SVR RMSE: {rmse_svr:.4f}')

# Random Forest Regressor Model
rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)
rfr_model.fit(X_train, y_train)
y_pred_rfr = rfr_model.predict(X_test)

# Calculate RMSE for Random Forest
rmse_rfr = np.sqrt(mean_squared_error(y_test, y_pred_rfr))
print(f'Random Forest RMSE: {rmse_rfr:.4f}')

# XGBoost Model
xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Calculate RMSE for XGBoost
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
print(f'XGBoost RMSE: {rmse_xgb:.4f}')